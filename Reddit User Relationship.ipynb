{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a482bf2c",
   "metadata": {},
   "source": [
    "# Social Media Group Project: Subreddit Homophily and Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b9760",
   "metadata": {},
   "source": [
    "## 1. Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae8e33",
   "metadata": {},
   "source": [
    "### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b1d4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import zstandard as zstd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cdb2d",
   "metadata": {},
   "source": [
    "### 1.2 Read compressed JSON files and read into list of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "946ed3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from DOG_comments.zst loaded successfully!\n",
      "Data from DOG_submissions.zst loaded successfully!\n",
      "Data from Pets_comments.zst loaded successfully!\n",
      "Data from Pets_submissions.zst loaded successfully!\n",
      "Data from puppies_comments.zst loaded successfully!\n",
      "Data from puppies_submissions.zst loaded successfully!\n",
      "Data from PuppySmiles_comments.zst loaded successfully!\n",
      "Data from PuppySmiles_submissions.zst loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def read_json_with_compression(filepath):\n",
    "    try:\n",
    "        # Decompressing the zstd file to JSON\n",
    "        with open(filepath, 'rb') as compressed:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            json_output_filepath = os.path.splitext(filepath)[0] + '.json'\n",
    "            with open(json_output_filepath, 'wb') as decompressed:\n",
    "                dctx.copy_stream(compressed, decompressed)\n",
    "        # Reading JSON into DataFrame\n",
    "        df = pd.read_json(json_output_filepath, lines=True)\n",
    "        print(f\"Data from {filepath} loaded successfully!\")\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading the JSON file at {filepath}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with file at {filepath}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Define file paths for each data set\n",
    "filepaths = {\n",
    "    'dog_comments': \"DOG_comments.zst\",\n",
    "    'dog_submissions': \"DOG_submissions.zst\",\n",
    "    'pets_comments': \"Pets_comments.zst\",\n",
    "    'pets_submissions':\"Pets_submissions.zst\",\n",
    "    'puppies_comments': \"puppies_comments.zst\",\n",
    "    'puppies_submissions': \"puppies_submissions.zst\",\n",
    "    'puppy_smiles_comments': \"PuppySmiles_comments.zst\",\n",
    "    'puppy_smiles_submissions': \"PuppySmiles_submissions.zst\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# Read data into dataframes\n",
    "for key, filepath in filepaths.items():\n",
    "    dataframes[key] = read_json_with_compression(filepath)\n",
    "\n",
    "# Access dataframes using keys\n",
    "df_dog_comments = dataframes['dog_comments']\n",
    "df_dog_submissions = dataframes['dog_submissions']\n",
    "df_pets_comments = dataframes['pets_comments']\n",
    "df_pets_submissions = dataframes['pets_submissions']\n",
    "df_puppies_comments = dataframes['puppies_comments']\n",
    "df_puppies_submissions = dataframes['puppies_submissions']\n",
    "df_puppy_smiles_comments = dataframes['puppy_smiles_comments']\n",
    "df_puppy_smiles_submissions = dataframes['puppy_smiles_submissions']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa28dd",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93448db",
   "metadata": {},
   "source": [
    "### 2.1 Check for missing ID's and define set of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1eed8225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'id' in dog_comments:\n",
      "Empty DataFrame\n",
      "Columns: [score_hidden, created_utc, name, ups, author_flair_text, edited, body, archived, parent_id, score, distinguished, controversiality, author_flair_css_class, gilded, link_id, retrieved_on, id, downs, subreddit_id, author, subreddit, removal_reason, stickied, can_gild, author_cakeday, approved_at_utc, can_mod_post, banned_at_utc, collapsed, collapsed_reason, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, approved_by, banned_by, body_html, likes, mod_reports, num_reports, replies, report_reasons, saved, user_reports, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 76 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in dog_submissions:\n",
      "Empty DataFrame\n",
      "Columns: [archived, author, author_flair_background_color, author_flair_css_class, author_flair_text, author_flair_text_color, brand_safe, can_gild, contest_mode, created_utc, distinguished, domain, edited, gilded, hidden, hide_score, id, is_crosspostable, is_reddit_media_domain, is_self, is_video, link_flair_css_class, link_flair_richtext, link_flair_text, link_flair_text_color, link_flair_type, locked, media, media_embed, no_follow, num_comments, num_crossposts, over_18, parent_whitelist_status, permalink, retrieved_on, rte_mode, score, secure_media, secure_media_embed, selftext, send_replies, spoiler, stickied, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type, suggested_sort, thumbnail, title, url, whitelist_status, author_flair_richtext, author_flair_type, thumbnail_height, thumbnail_width, author_cakeday, is_original_content, downs, num_reports, banned_by, name, likes, clicked, saved, ups, approved_by, selftext_html, created, report_reasons, mod_reports, user_reports, quarantine, from_kind, from_id, from, preview, post_hint, view_count, approved_at_utc, banned_at_utc, can_mod_post, pinned, crosspost_parent, crosspost_parent_list, mod_note, mod_reason_by, mod_reason_title, subreddit_subscribers, author_flair_template_id, category, content_categories, media_only, post_categories, pwls, removal_reason, visited, wls, media_metadata, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 127 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in pets_comments:\n",
      "Empty DataFrame\n",
      "Columns: [score, author, archived, ups, subreddit_id, id, retrieved_on, link_id, downs, controversiality, body, edited, distinguished, author_flair_css_class, created_utc, subreddit, author_flair_text, name, score_hidden, parent_id, gilded, removal_reason, likes, created, banned_by, body_html, user_reports, replies, approved_by, report_reasons, saved, mod_reports, num_reports, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 77 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in pets_submissions:\n",
      "Empty DataFrame\n",
      "Columns: [archived, author, author_flair_background_color, author_flair_css_class, author_flair_richtext, author_flair_text, author_flair_text_color, author_flair_type, brand_safe, can_gild, contest_mode, created_utc, distinguished, domain, edited, gilded, hidden, hide_score, id, is_crosspostable, is_reddit_media_domain, is_self, is_video, link_flair_css_class, link_flair_richtext, link_flair_text, link_flair_text_color, link_flair_type, locked, media, media_embed, no_follow, num_comments, num_crossposts, over_18, parent_whitelist_status, permalink, retrieved_on, rte_mode, score, secure_media, secure_media_embed, selftext, send_replies, spoiler, stickied, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type, suggested_sort, thumbnail, thumbnail_height, thumbnail_width, title, url, whitelist_status, author_cakeday, post_hint, preview, is_original_content, downs, num_reports, banned_by, name, likes, clicked, saved, ups, approved_by, selftext_html, created, user_reports, mod_reports, report_reasons, from, from_id, from_kind, quarantine, view_count, approved_at_utc, banned_at_utc, can_mod_post, pinned, crosspost_parent, crosspost_parent_list, mod_note, mod_reason_by, mod_reason_title, subreddit_subscribers, author_flair_template_id, category, content_categories, media_only, post_categories, pwls, removal_reason, visited, wls, previous_visits, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 127 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in puppies_comments:\n",
      "Empty DataFrame\n",
      "Columns: [author_flair_css_class, distinguished, archived, id, controversiality, ups, subreddit, retrieved_on, author, downs, edited, score, score_hidden, name, gilded, body, author_flair_text, link_id, subreddit_id, parent_id, created_utc, removal_reason, saved, report_reasons, num_reports, mod_reports, created, likes, user_reports, replies, approved_by, body_html, banned_by, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 76 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in puppies_submissions:\n",
      "Empty DataFrame\n",
      "Columns: [archived, author, author_flair_background_color, author_flair_css_class, author_flair_richtext, author_flair_text, author_flair_text_color, author_flair_type, brand_safe, can_gild, contest_mode, created_utc, distinguished, domain, edited, gilded, hidden, hide_score, id, is_crosspostable, is_reddit_media_domain, is_self, is_video, link_flair_css_class, link_flair_richtext, link_flair_text, link_flair_text_color, link_flair_type, locked, media, media_embed, no_follow, num_comments, num_crossposts, over_18, parent_whitelist_status, permalink, retrieved_on, rte_mode, score, secure_media, secure_media_embed, selftext, send_replies, spoiler, stickied, subreddit, subreddit_id, subreddit_name_prefixed, subreddit_type, suggested_sort, thumbnail, title, url, whitelist_status, thumbnail_height, thumbnail_width, is_original_content, downs, num_reports, banned_by, name, likes, clicked, saved, ups, approved_by, selftext_html, created, mod_reports, user_reports, report_reasons, from, from_id, from_kind, quarantine, post_hint, preview, author_cakeday, view_count, approved_at_utc, banned_at_utc, can_mod_post, pinned, crosspost_parent, crosspost_parent_list, mod_note, mod_reason_by, mod_reason_title, subreddit_subscribers, author_flair_template_id, category, content_categories, media_only, post_categories, pwls, removal_reason, visited, wls, previous_visits, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 127 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in puppy_smiles_comments:\n",
      "Empty DataFrame\n",
      "Columns: [edited, link_id, archived, author_flair_text, ups, created_utc, author_flair_css_class, author, parent_id, score, id, gilded, distinguished, subreddit, controversiality, retrieved_on, downs, subreddit_id, name, body, score_hidden, replies, likes, body_html, banned_by, saved, user_reports, mod_reports, created, report_reasons, num_reports, approved_by, removal_reason, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, subreddit_name_prefixed, author_created_utc, author_fullname, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 77 columns]\n",
      "\n",
      "\n",
      "Missing 'id' in puppy_smiles_submissions:\n",
      "Empty DataFrame\n",
      "Columns: [link_flair_text, permalink, selftext, media, ups, retrieved_on, created_utc, author_flair_text, media_embed, subreddit, domain, hide_score, downs, thumbnail, distinguished, title, secure_media_embed, from_id, secure_media, url, created, from_kind, subreddit_id, stickied, score, edited, quarantine, author_flair_css_class, num_comments, saved, over_18, link_flair_css_class, gilded, name, archived, id, from, author, is_self, post_hint, preview, locked, contest_mode, spoiler, brand_safe, suggested_sort, hidden, view_count, thumbnail_width, can_gild, thumbnail_height, is_video, approved_at_utc, banned_at_utc, can_mod_post, is_crosspostable, num_crossposts, parent_whitelist_status, whitelist_status, author_cakeday, is_reddit_media_domain, pinned, subreddit_type, crosspost_parent, crosspost_parent_list, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, subreddit_subscribers, approved_by, author_flair_background_color, author_flair_richtext, author_flair_template_id, author_flair_text_color, author_flair_type, banned_by, category, clicked, content_categories, is_original_content, likes, link_flair_richtext, link_flair_text_color, link_flair_type, media_only, mod_reports, num_reports, post_categories, pwls, removal_reason, report_reasons, rte_mode, selftext_html, user_reports, visited, wls, link_flair_background_color, link_flair_template_id, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 126 columns]\n",
      "\n",
      "\n",
      "Missing 'link_id' in dog_comments:\n",
      "Empty DataFrame\n",
      "Columns: [score_hidden, created_utc, name, ups, author_flair_text, edited, body, archived, parent_id, score, distinguished, controversiality, author_flair_css_class, gilded, link_id, retrieved_on, id, downs, subreddit_id, author, subreddit, removal_reason, stickied, can_gild, author_cakeday, approved_at_utc, can_mod_post, banned_at_utc, collapsed, collapsed_reason, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, approved_by, banned_by, body_html, likes, mod_reports, num_reports, replies, report_reasons, saved, user_reports, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 76 columns]\n",
      "\n",
      "\n",
      "Missing 'link_id' in pets_comments:\n",
      "Empty DataFrame\n",
      "Columns: [score, author, archived, ups, subreddit_id, id, retrieved_on, link_id, downs, controversiality, body, edited, distinguished, author_flair_css_class, created_utc, subreddit, author_flair_text, name, score_hidden, parent_id, gilded, removal_reason, likes, created, banned_by, body_html, user_reports, replies, approved_by, report_reasons, saved, mod_reports, num_reports, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 77 columns]\n",
      "\n",
      "\n",
      "Missing 'link_id' in puppies_comments:\n",
      "Empty DataFrame\n",
      "Columns: [author_flair_css_class, distinguished, archived, id, controversiality, ups, subreddit, retrieved_on, author, downs, edited, score, score_hidden, name, gilded, body, author_flair_text, link_id, subreddit_id, parent_id, created_utc, removal_reason, saved, report_reasons, num_reports, mod_reports, created, likes, user_reports, replies, approved_by, body_html, banned_by, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, author_created_utc, author_fullname, subreddit_name_prefixed, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 76 columns]\n",
      "\n",
      "\n",
      "Missing 'link_id' in puppy_smiles_comments:\n",
      "Empty DataFrame\n",
      "Columns: [edited, link_id, archived, author_flair_text, ups, created_utc, author_flair_css_class, author, parent_id, score, id, gilded, distinguished, subreddit, controversiality, retrieved_on, downs, subreddit_id, name, body, score_hidden, replies, likes, body_html, banned_by, saved, user_reports, mod_reports, created, report_reasons, num_reports, approved_by, removal_reason, stickied, author_cakeday, can_gild, approved_at_utc, can_mod_post, collapsed, collapsed_reason, banned_at_utc, is_submitter, permalink, subreddit_type, mod_note, mod_reason_by, mod_reason_title, no_follow, send_replies, author_flair_template_id, author_flair_background_color, author_flair_richtext, author_flair_text_color, author_flair_type, rte_mode, subreddit_name_prefixed, author_created_utc, author_fullname, gildings, author_patreon_flair, quarantined, locked, all_awardings, total_awards_received, steward_reports, awarders, associated_award, collapsed_because_crowd_control, author_premium, treatment_tags, top_awarded_type, comment_type, collapsed_reason_code, retrieved_utc, author_is_blocked, unrepliable_reason, editable]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 77 columns]\n",
      "\n",
      "\n",
      "Head of df_dog_comments:\n",
      "   score_hidden  created_utc        name  ups author_flair_text  edited  \\\n",
      "0           0.0   1234980795  t1_c07qst3  1.0              None       0   \n",
      "1           0.0   1238166380  t1_c08hxft  2.0              None       0   \n",
      "2           0.0   1238184156  t1_c08i744  1.0              None       0   \n",
      "3           0.0   1239248453  t1_c08qub1  1.0              None       0   \n",
      "4           0.0   1241738842  t1_c09eubj  2.0              None       0   \n",
      "\n",
      "                                                body  archived   parent_id  \\\n",
      "0  With so much negative news lately, I thought i...       1.0    t3_7yesg   \n",
      "1  I agree with you. I haven't looking for a pupp...       1.0    t3_87w8g   \n",
      "2  My girlfriend and I just rescued to pups (Boxa...       1.0  t1_c08hxft   \n",
      "3  Yeah, thats just selling dogs. Anything over 3...       1.0    t3_87w8g   \n",
      "4  Let's see - add up the cost of spay/neuter, va...       1.0    t3_87w8g   \n",
      "\n",
      "   score  ... collapsed_because_crowd_control  author_premium treatment_tags  \\\n",
      "0      1  ...                             NaN             NaN            NaN   \n",
      "1      2  ...                             NaN             NaN            NaN   \n",
      "2      1  ...                             NaN             NaN            NaN   \n",
      "3      1  ...                             NaN             NaN            NaN   \n",
      "4      2  ...                             NaN             NaN            NaN   \n",
      "\n",
      "   top_awarded_type comment_type  collapsed_reason_code retrieved_utc  \\\n",
      "0               NaN          NaN                    NaN           NaN   \n",
      "1               NaN          NaN                    NaN           NaN   \n",
      "2               NaN          NaN                    NaN           NaN   \n",
      "3               NaN          NaN                    NaN           NaN   \n",
      "4               NaN          NaN                    NaN           NaN   \n",
      "\n",
      "   author_is_blocked unrepliable_reason editable  \n",
      "0                NaN                NaN      NaN  \n",
      "1                NaN                NaN      NaN  \n",
      "2                NaN                NaN      NaN  \n",
      "3                NaN                NaN      NaN  \n",
      "4                NaN                NaN      NaN  \n",
      "\n",
      "[5 rows x 76 columns]\n",
      "\n",
      "Head of df_dog_submissions:\n",
      "   archived       author author_flair_background_color author_flair_css_class  \\\n",
      "0       1.0    [deleted]                                                 None   \n",
      "1       1.0    [deleted]                                                 None   \n",
      "2       1.0         bugg                          None                   None   \n",
      "3       1.0    [deleted]                                                 None   \n",
      "4       1.0  karenanddot                          None                   None   \n",
      "\n",
      "  author_flair_text author_flair_text_color  brand_safe  can_gild  \\\n",
      "0              None                    dark         0.0       0.0   \n",
      "1              None                    dark         0.0       0.0   \n",
      "2              None                    None         0.0       1.0   \n",
      "3              None                    dark         0.0       0.0   \n",
      "4              None                    None         0.0       1.0   \n",
      "\n",
      "   contest_mode  created_utc  ... treatment_tags poll_data  upvote_ratio  \\\n",
      "0           0.0   1227059352  ...            NaN       NaN           NaN   \n",
      "1           0.0   1227061301  ...            NaN       NaN           NaN   \n",
      "2           0.0   1232073680  ...            NaN       NaN           NaN   \n",
      "3           0.0   1233865599  ...            NaN       NaN           NaN   \n",
      "4           0.0   1234980764  ...            NaN       NaN           NaN   \n",
      "\n",
      "   is_created_from_ads_ui  retrieved_utc  top_awarded_type  \\\n",
      "0                     NaN            NaN               NaN   \n",
      "1                     NaN            NaN               NaN   \n",
      "2                     NaN            NaN               NaN   \n",
      "3                     NaN            NaN               NaN   \n",
      "4                     NaN            NaN               NaN   \n",
      "\n",
      "  url_overridden_by_dest  gallery_data  is_gallery  call_to_action  \n",
      "0                    NaN           NaN         NaN             NaN  \n",
      "1                    NaN           NaN         NaN             NaN  \n",
      "2                    NaN           NaN         NaN             NaN  \n",
      "3                    NaN           NaN         NaN             NaN  \n",
      "4                    NaN           NaN         NaN             NaN  \n",
      "\n",
      "[5 rows x 127 columns]\n",
      "\n",
      "Head of df_pets_comments:\n",
      "   score          author  archived  ups subreddit_id       id  retrieved_on  \\\n",
      "0      1  AngelaMotorman       1.0  1.0     t5_2qh6o  c030v0n  1.425825e+09   \n",
      "1      1  AngelaMotorman       1.0  1.0     t5_2qh6o  c032b20  1.425826e+09   \n",
      "2      1        MrKlaatu       1.0  1.0     t5_2qh6o  c033qsd  1.425828e+09   \n",
      "3      1  AngelaMotorman       1.0  1.0     t5_2qh6o  c033qyo  1.425828e+09   \n",
      "4      1       [deleted]       1.0  1.0     t5_2qh6o  c035rw5  1.425829e+09   \n",
      "\n",
      "    link_id  downs  controversiality  ... collapsed_because_crowd_control  \\\n",
      "0  t3_66obl    0.0                 0  ...                             NaN   \n",
      "1  t3_67dha    0.0                 0  ...                             NaN   \n",
      "2  t3_67t9g    0.0                 0  ...                             NaN   \n",
      "3  t3_67t9g    0.0                 0  ...                             NaN   \n",
      "4  t3_68fky    0.0                 0  ...                             NaN   \n",
      "\n",
      "   author_premium treatment_tags top_awarded_type  comment_type  \\\n",
      "0             NaN            NaN              NaN           NaN   \n",
      "1             NaN            NaN              NaN           NaN   \n",
      "2             NaN            NaN              NaN           NaN   \n",
      "3             NaN            NaN              NaN           NaN   \n",
      "4             NaN            NaN              NaN           NaN   \n",
      "\n",
      "  collapsed_reason_code retrieved_utc author_is_blocked  unrepliable_reason  \\\n",
      "0                   NaN           NaN               NaN                 NaN   \n",
      "1                   NaN           NaN               NaN                 NaN   \n",
      "2                   NaN           NaN               NaN                 NaN   \n",
      "3                   NaN           NaN               NaN                 NaN   \n",
      "4                   NaN           NaN               NaN                 NaN   \n",
      "\n",
      "  editable  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n",
      "\n",
      "[5 rows x 77 columns]\n",
      "\n",
      "Head of df_pets_submissions:\n",
      "   archived    author author_flair_background_color author_flair_css_class  \\\n",
      "0       1.0  neoronin                          None                   None   \n",
      "1       1.0  neoronin                          None                   None   \n",
      "2       1.0  neoronin                          None                   None   \n",
      "3       1.0  neoronin                          None                   None   \n",
      "4       1.0  neoronin                          None                   None   \n",
      "\n",
      "  author_flair_richtext author_flair_text author_flair_text_color  \\\n",
      "0                    []              None                    None   \n",
      "1                    []              None                    None   \n",
      "2                    []              None                    None   \n",
      "3                    []              None                    None   \n",
      "4                    []              None                    None   \n",
      "\n",
      "  author_flair_type  brand_safe  can_gild  ...  treatment_tags  poll_data  \\\n",
      "0              text         1.0       1.0  ...             NaN        NaN   \n",
      "1              text         1.0       1.0  ...             NaN        NaN   \n",
      "2              text         1.0       1.0  ...             NaN        NaN   \n",
      "3              text         1.0       1.0  ...             NaN        NaN   \n",
      "4              text         1.0       1.0  ...             NaN        NaN   \n",
      "\n",
      "  upvote_ratio is_created_from_ads_ui  retrieved_utc  top_awarded_type  \\\n",
      "0          NaN                    NaN            NaN               NaN   \n",
      "1          NaN                    NaN            NaN               NaN   \n",
      "2          NaN                    NaN            NaN               NaN   \n",
      "3          NaN                    NaN            NaN               NaN   \n",
      "4          NaN                    NaN            NaN               NaN   \n",
      "\n",
      "   url_overridden_by_dest  gallery_data is_gallery  call_to_action  \n",
      "0                     NaN           NaN        NaN             NaN  \n",
      "1                     NaN           NaN        NaN             NaN  \n",
      "2                     NaN           NaN        NaN             NaN  \n",
      "3                     NaN           NaN        NaN             NaN  \n",
      "4                     NaN           NaN        NaN             NaN  \n",
      "\n",
      "[5 rows x 127 columns]\n",
      "\n",
      "Head of df_puppies_comments:\n",
      "   author_flair_css_class distinguished  archived       id  controversiality  \\\n",
      "0                     NaN          None       1.0  c076dju                 0   \n",
      "1                     NaN          None       1.0  c0jmufv                 0   \n",
      "2                     NaN          None       1.0  c0v55r7                 0   \n",
      "3                     NaN          None       1.0  c13medd                 0   \n",
      "4                     NaN          None       1.0  c1iaafq                 0   \n",
      "\n",
      "   ups subreddit  retrieved_on        author  downs  ...  associated_award  \\\n",
      "0  1.0   puppies  1.425915e+09     [deleted]    0.0  ...               NaN   \n",
      "1  1.0   puppies  1.426199e+09  gusbustafunk    0.0  ...               NaN   \n",
      "2  1.0   puppies  1.426394e+09       Mari181    0.0  ...               NaN   \n",
      "3  1.0   puppies  1.426539e+09      soulwarp    0.0  ...               NaN   \n",
      "4  1.0   puppies  1.426943e+09  theleftenant    0.0  ...               NaN   \n",
      "\n",
      "   collapsed_because_crowd_control  author_premium treatment_tags  \\\n",
      "0                              NaN             NaN            NaN   \n",
      "1                              NaN             NaN            NaN   \n",
      "2                              NaN             NaN            NaN   \n",
      "3                              NaN             NaN            NaN   \n",
      "4                              NaN             NaN            NaN   \n",
      "\n",
      "   top_awarded_type comment_type  collapsed_reason_code retrieved_utc  \\\n",
      "0               NaN          NaN                    NaN           NaN   \n",
      "1               NaN          NaN                    NaN           NaN   \n",
      "2               NaN          NaN                    NaN           NaN   \n",
      "3               NaN          NaN                    NaN           NaN   \n",
      "4               NaN          NaN                    NaN           NaN   \n",
      "\n",
      "  author_is_blocked unrepliable_reason  \n",
      "0               NaN                NaN  \n",
      "1               NaN                NaN  \n",
      "2               NaN                NaN  \n",
      "3               NaN                NaN  \n",
      "4               NaN                NaN  \n",
      "\n",
      "[5 rows x 76 columns]\n",
      "\n",
      "Head of df_puppies_submissions:\n",
      "   archived  author author_flair_background_color author_flair_css_class  \\\n",
      "0       1.0  mrawde                          None                   None   \n",
      "1       1.0  mrawde                          None                   None   \n",
      "2       1.0  mrawde                          None                   None   \n",
      "3       1.0  mrawde                          None                   None   \n",
      "4       1.0  mrawde                          None                   None   \n",
      "\n",
      "  author_flair_richtext author_flair_text author_flair_text_color  \\\n",
      "0                    []              None                    None   \n",
      "1                    []              None                    None   \n",
      "2                    []              None                    None   \n",
      "3                    []              None                    None   \n",
      "4                    []              None                    None   \n",
      "\n",
      "  author_flair_type  brand_safe  can_gild  ...  treatment_tags  poll_data  \\\n",
      "0              text         1.0       1.0  ...             NaN        NaN   \n",
      "1              text         1.0       1.0  ...             NaN        NaN   \n",
      "2              text         1.0       1.0  ...             NaN        NaN   \n",
      "3              text         1.0       1.0  ...             NaN        NaN   \n",
      "4              text         1.0       1.0  ...             NaN        NaN   \n",
      "\n",
      "   upvote_ratio is_created_from_ads_ui  retrieved_utc  top_awarded_type  \\\n",
      "0           NaN                    NaN            NaN               NaN   \n",
      "1           NaN                    NaN            NaN               NaN   \n",
      "2           NaN                    NaN            NaN               NaN   \n",
      "3           NaN                    NaN            NaN               NaN   \n",
      "4           NaN                    NaN            NaN               NaN   \n",
      "\n",
      "   url_overridden_by_dest  gallery_data is_gallery  call_to_action  \n",
      "0                     NaN           NaN        NaN             NaN  \n",
      "1                     NaN           NaN        NaN             NaN  \n",
      "2                     NaN           NaN        NaN             NaN  \n",
      "3                     NaN           NaN        NaN             NaN  \n",
      "4                     NaN           NaN        NaN             NaN  \n",
      "\n",
      "[5 rows x 127 columns]\n",
      "\n",
      "Head of df_puppy_smiles_comments:\n",
      "   edited    link_id  archived author_flair_text  ups  created_utc  \\\n",
      "0       0  t3_2kabjp       0.0                    3.0   1414263180   \n",
      "1       0  t3_2kmvgz       0.0                    2.0   1414849552   \n",
      "2       0  t3_2l5rnx       0.0                    2.0   1415029440   \n",
      "3       0  t3_2l5rnx       0.0                    2.0   1415031033   \n",
      "4       0  t3_2l626z       0.0                    4.0   1415036216   \n",
      "\n",
      "  author_flair_css_class         author  parent_id  score  ...  \\\n",
      "0                   ps14      dystopika  t3_2kabjp      3  ...   \n",
      "1                   ps14      dystopika  t3_2kmvgz      2  ...   \n",
      "2                   ps03  love_the_heat  t3_2l5rnx      2  ...   \n",
      "3                   ps14      dystopika  t3_2l5rnx      2  ...   \n",
      "4                   ps03  love_the_heat  t3_2l626z      4  ...   \n",
      "\n",
      "  collapsed_because_crowd_control  author_premium treatment_tags  \\\n",
      "0                             NaN             NaN            NaN   \n",
      "1                             NaN             NaN            NaN   \n",
      "2                             NaN             NaN            NaN   \n",
      "3                             NaN             NaN            NaN   \n",
      "4                             NaN             NaN            NaN   \n",
      "\n",
      "  top_awarded_type  comment_type  collapsed_reason_code  retrieved_utc  \\\n",
      "0              NaN           NaN                    NaN            NaN   \n",
      "1              NaN           NaN                    NaN            NaN   \n",
      "2              NaN           NaN                    NaN            NaN   \n",
      "3              NaN           NaN                    NaN            NaN   \n",
      "4              NaN           NaN                    NaN            NaN   \n",
      "\n",
      "  author_is_blocked unrepliable_reason editable  \n",
      "0               NaN                NaN      NaN  \n",
      "1               NaN                NaN      NaN  \n",
      "2               NaN                NaN      NaN  \n",
      "3               NaN                NaN      NaN  \n",
      "4               NaN                NaN      NaN  \n",
      "\n",
      "[5 rows x 77 columns]\n",
      "\n",
      "Head of df_puppy_smiles_submissions:\n",
      "  link_flair_text                                          permalink selftext  \\\n",
      "0            None  /r/PuppySmiles/comments/2kaa1x/such_a_nice_smile/            \n",
      "1            None   /r/PuppySmiles/comments/2kaa6l/very_happy_puppy/            \n",
      "2            None            /r/PuppySmiles/comments/2kaamy/so_cute/            \n",
      "3            None  /r/PuppySmiles/comments/2kaaq0/adorable_happy_...            \n",
      "4            None  /r/PuppySmiles/comments/2kaaud/so_happy_to_be_...            \n",
      "\n",
      "  media   ups  retrieved_on  created_utc author_flair_text media_embed  \\\n",
      "0  None  13.0  1.441105e+09   1414244976                            {}   \n",
      "1  None  15.0  1.441105e+09   1414245072                            {}   \n",
      "2  None  25.0  1.441105e+09   1414245462                            {}   \n",
      "3  None  18.0  1.441105e+09   1414245543                            {}   \n",
      "4  None  23.0  1.441105e+09   1414245632                            {}   \n",
      "\n",
      "     subreddit  ... treatment_tags  upvote_ratio  is_created_from_ads_ui  \\\n",
      "0  PuppySmiles  ...            NaN           NaN                     NaN   \n",
      "1  PuppySmiles  ...            NaN           NaN                     NaN   \n",
      "2  PuppySmiles  ...            NaN           NaN                     NaN   \n",
      "3  PuppySmiles  ...            NaN           NaN                     NaN   \n",
      "4  PuppySmiles  ...            NaN           NaN                     NaN   \n",
      "\n",
      "  retrieved_utc top_awarded_type url_overridden_by_dest gallery_data  \\\n",
      "0           NaN              NaN                    NaN          NaN   \n",
      "1           NaN              NaN                    NaN          NaN   \n",
      "2           NaN              NaN                    NaN          NaN   \n",
      "3           NaN              NaN                    NaN          NaN   \n",
      "4           NaN              NaN                    NaN          NaN   \n",
      "\n",
      "   is_gallery media_metadata call_to_action  \n",
      "0         NaN            NaN            NaN  \n",
      "1         NaN            NaN            NaN  \n",
      "2         NaN            NaN            NaN  \n",
      "3         NaN            NaN            NaN  \n",
      "4         NaN            NaN            NaN  \n",
      "\n",
      "[5 rows x 126 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of DataFrames\n",
    "dataframes = {\n",
    "    'dog_comments': df_dog_comments,\n",
    "    'dog_submissions': df_dog_submissions,\n",
    "    'pets_comments': df_pets_comments,\n",
    "    'pets_submissions': df_pets_submissions,\n",
    "    'puppies_comments': df_puppies_comments,\n",
    "    'puppies_submissions': df_puppies_submissions,\n",
    "    'puppy_smiles_comments': df_puppy_smiles_comments,\n",
    "    'puppy_smiles_submissions': df_puppy_smiles_submissions\n",
    "}\n",
    "\n",
    "# Check for missing 'id' in each DataFrame\n",
    "missing_ids = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_ids[name] = df[df['id'].isnull()]\n",
    "\n",
    "# Print results\n",
    "for name, df_missing_ids in missing_ids.items():\n",
    "    print(f\"Missing 'id' in {name}:\")\n",
    "    print(df_missing_ids)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# List of DataFrames for comments\n",
    "dataframes_comments = {\n",
    "    'dog_comments': df_dog_comments,\n",
    "    'pets_comments': df_pets_comments,\n",
    "    'puppies_comments': df_puppies_comments,\n",
    "    'puppy_smiles_comments': df_puppy_smiles_comments\n",
    "}\n",
    "\n",
    "# Check for missing 'link_id' in each DataFrame\n",
    "missing_ids_comments = {}\n",
    "for name, df in dataframes_comments.items():\n",
    "    missing_ids_comments[name] = df[df['link_id'].isnull()]\n",
    "\n",
    "# Print results\n",
    "for name, df_missing_ids in missing_ids_comments.items():\n",
    "    print(f\"Missing 'link_id' in {name}:\")\n",
    "    print(df_missing_ids)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Access dataframes using keys and get the head\n",
    "df_dog_comments = dataframes['dog_comments']\n",
    "print(\"Head of df_dog_comments:\")\n",
    "print(df_dog_comments.head())\n",
    "\n",
    "df_dog_submissions = dataframes['dog_submissions']\n",
    "print(\"\\nHead of df_dog_submissions:\")\n",
    "print(df_dog_submissions.head())\n",
    "\n",
    "df_pets_comments = dataframes['pets_comments']\n",
    "print(\"\\nHead of df_pets_comments:\")\n",
    "print(df_pets_comments.head())\n",
    "\n",
    "df_pets_submissions = dataframes['pets_submissions']\n",
    "print(\"\\nHead of df_pets_submissions:\")\n",
    "print(df_pets_submissions.head())\n",
    "\n",
    "df_puppies_comments = dataframes['puppies_comments']\n",
    "print(\"\\nHead of df_puppies_comments:\")\n",
    "print(df_puppies_comments.head())\n",
    "\n",
    "df_puppies_submissions = dataframes['puppies_submissions']\n",
    "print(\"\\nHead of df_puppies_submissions:\")\n",
    "print(df_puppies_submissions.head())\n",
    "\n",
    "df_puppy_smiles_comments = dataframes['puppy_smiles_comments']\n",
    "print(\"\\nHead of df_puppy_smiles_comments:\")\n",
    "print(df_puppy_smiles_comments.head())\n",
    "\n",
    "df_puppy_smiles_submissions = dataframes['puppy_smiles_submissions']\n",
    "print(\"\\nHead of df_puppy_smiles_submissions:\")\n",
    "print(df_puppy_smiles_submissions.head())\n",
    "\n",
    "# List of DataFrames (updated)\n",
    "dataframes = {\n",
    "    'dog_comments': df_dog_comments,\n",
    "    'dog_submissions': df_dog_submissions,\n",
    "    'pets_comments': df_pets_comments,\n",
    "    'pets_submissions': df_pets_submissions,\n",
    "    'puppies_comments': df_puppies_comments,\n",
    "    'puppies_submissions': df_puppies_submissions,\n",
    "    'puppy_smiles_comments': df_puppy_smiles_comments,\n",
    "    'puppy_smiles_submissions': df_puppy_smiles_submissions\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e729f",
   "metadata": {},
   "source": [
    "### 2.2 Convert unix timestamp to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33630dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct years: 15\n",
      "Distinct years: [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'created_utc' from UNIX timestamp to datetime for each dataframe\n",
    "for df_name, df in dataframes.items():\n",
    "    df['created_datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Initialize an empty set to store unique years\n",
    "distinct_years = set()\n",
    "\n",
    "# Iterate through each DataFrame and extract unique years\n",
    "for df_name, df in dataframes.items():\n",
    "    distinct_years.update(df['created_datetime'].dt.year.unique())\n",
    "\n",
    "# Print the number of distinct years\n",
    "print(\"Number of distinct years:\", len(distinct_years))\n",
    "\n",
    "# Print the distinct years found\n",
    "print(\"Distinct years:\", sorted(distinct_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becabe04",
   "metadata": {},
   "source": [
    "### 2.3 Filter data by base year (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "659201f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_2017_dog_comments_filtered = df_base_year_filtered_dataframes['df_2017_dog_comments_filtered']\n",
      "df_2017_dog_submissions_filtered = df_base_year_filtered_dataframes['df_2017_dog_submissions_filtered']\n",
      "df_2017_pets_comments_filtered = df_base_year_filtered_dataframes['df_2017_pets_comments_filtered']\n",
      "df_2017_pets_submissions_filtered = df_base_year_filtered_dataframes['df_2017_pets_submissions_filtered']\n",
      "df_2017_puppies_comments_filtered = df_base_year_filtered_dataframes['df_2017_puppies_comments_filtered']\n",
      "df_2017_puppies_submissions_filtered = df_base_year_filtered_dataframes['df_2017_puppies_submissions_filtered']\n",
      "df_2017_puppy_smiles_comments_filtered = df_base_year_filtered_dataframes['df_2017_puppy_smiles_comments_filtered']\n",
      "df_2017_puppy_smiles_submissions_filtered = df_base_year_filtered_dataframes['df_2017_puppy_smiles_submissions_filtered']\n"
     ]
    }
   ],
   "source": [
    "# Define base year\n",
    "base_year = 2017\n",
    "\n",
    "# Filter each dataframe to include only the base year\n",
    "filtered_dataframes = {}\n",
    "for df_name, df in dataframes.items():\n",
    "    filtered_dataframes[df_name] = df[df['created_datetime'].dt.year == base_year]\n",
    "\n",
    "# Filter out rows with author as '[deleted]' for each DataFrame\n",
    "filtered_dataframes_no_deleted = {}\n",
    "for name, df in filtered_dataframes.items():\n",
    "    filtered_dataframes_no_deleted[name] = df[df['author'] != '[deleted]']\n",
    "\n",
    "# Access filtered DataFrames, create new ones with the year in front of the name\n",
    "df_base_year_filtered_dataframes = {}\n",
    "for name, df in filtered_dataframes_no_deleted.items():\n",
    "    df_base_year_filtered_dataframes[f\"df_{base_year}_{name}_filtered\"] = df\n",
    "\n",
    "# Access filtered DataFrames\n",
    "for name, df in df_base_year_filtered_dataframes.items():\n",
    "    print(f\"{name} = df_base_year_filtered_dataframes['{name}']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00943f",
   "metadata": {},
   "source": [
    "### 2.4 Filter data by comparison year (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5526d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_2019_dog_comments_filtered = df_comparison_year_filtered_dataframes['df_2019_dog_comments_filtered']\n",
      "df_2019_dog_submissions_filtered = df_comparison_year_filtered_dataframes['df_2019_dog_submissions_filtered']\n",
      "df_2019_pets_comments_filtered = df_comparison_year_filtered_dataframes['df_2019_pets_comments_filtered']\n",
      "df_2019_pets_submissions_filtered = df_comparison_year_filtered_dataframes['df_2019_pets_submissions_filtered']\n",
      "df_2019_puppies_comments_filtered = df_comparison_year_filtered_dataframes['df_2019_puppies_comments_filtered']\n",
      "df_2019_puppies_submissions_filtered = df_comparison_year_filtered_dataframes['df_2019_puppies_submissions_filtered']\n",
      "df_2019_puppy_smiles_comments_filtered = df_comparison_year_filtered_dataframes['df_2019_puppy_smiles_comments_filtered']\n",
      "df_2019_puppy_smiles_submissions_filtered = df_comparison_year_filtered_dataframes['df_2019_puppy_smiles_submissions_filtered']\n"
     ]
    }
   ],
   "source": [
    "# Define comparison year\n",
    "comparison_year = 2019\n",
    "\n",
    "# Filter each dataframe to include only the comparison year\n",
    "filtered_dataframes = {}\n",
    "for df_name, df in dataframes.items():\n",
    "    filtered_dataframes[df_name] = df[df['created_datetime'].dt.year == comparison_year]\n",
    "\n",
    "# Filter out rows with author as '[deleted]' for each DataFrame\n",
    "filtered_dataframes_no_deleted = {}\n",
    "for name, df in filtered_dataframes.items():\n",
    "    filtered_dataframes_no_deleted[name] = df[df['author'] != '[deleted]']\n",
    "\n",
    "# Access filtered DataFrames, create new ones with the year in front of the name\n",
    "df_comparison_year_filtered_dataframes = {}\n",
    "for name, df in filtered_dataframes_no_deleted.items():\n",
    "    df_comparison_year_filtered_dataframes[f\"df_{comparison_year}_{name}_filtered\"] = df\n",
    "\n",
    "# Access filtered DataFrames\n",
    "for name, df in df_comparison_year_filtered_dataframes.items():\n",
    "    print(f\"{name} = df_comparison_year_filtered_dataframes['{name}']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4271644",
   "metadata": {},
   "source": [
    "## 3. Activity Analysis and Top User Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52ea63",
   "metadata": {},
   "source": [
    "This section of code performs an analysis of user activity based on submissions and comments for a specific year. It calculates the total activity for each user by summing the number of posts and comments, then identifies the top 20 most active users. Finally, it generates a combination DataFrame of the top users for further analysis.\n",
    "\n",
    "The combination DataFrame generated in the provided code represents all possible pairs of users among the top 20 most active users. Each row in the DataFrame contains a pair of users where one user is considered the \"Parent ID\" and the other user is considered the \"Child ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa1d9f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                author  created_datetime  post_count  comment_count  \\\n",
      "689        Tillmantino              2017       334.0            0.0   \n",
      "1805     AutoModerator              2017         0.0          154.0   \n",
      "2             0010pond              2017        53.0            0.0   \n",
      "1997     imguralbumbot              2017         0.0           45.0   \n",
      "941      crazydogsinfo              2017        30.0            0.0   \n",
      "504      NowRecyclable              2017        11.0           18.0   \n",
      "559       Puppuniverse              2017        26.0            0.0   \n",
      "1779            zanliu              2017        20.0            4.0   \n",
      "1641  squirrelinstinct              2017         3.0           17.0   \n",
      "805            antdude              2017        20.0            0.0   \n",
      "576            Repreve              2017        11.0            7.0   \n",
      "1801          Archaris              2017         0.0           18.0   \n",
      "785        allenmonkey              2017        14.0            0.0   \n",
      "804          ankitwish              2017        11.0            1.0   \n",
      "1633      sosyal-medya              2017        11.0            0.0   \n",
      "273      Gladiatordogs              2017         7.0            4.0   \n",
      "1047    ethansmith1409              2017        11.0            0.0   \n",
      "1203         jang_paul              2017        11.0            0.0   \n",
      "2035        nicedogbot              2017         0.0           11.0   \n",
      "14          4shareapks              2017        10.0            0.0   \n",
      "\n",
      "      total_activity  \n",
      "689            334.0  \n",
      "1805           154.0  \n",
      "2               53.0  \n",
      "1997            45.0  \n",
      "941             30.0  \n",
      "504             29.0  \n",
      "559             26.0  \n",
      "1779            24.0  \n",
      "1641            20.0  \n",
      "805             20.0  \n",
      "576             18.0  \n",
      "1801            18.0  \n",
      "785             14.0  \n",
      "804             12.0  \n",
      "1633            11.0  \n",
      "273             11.0  \n",
      "1047            11.0  \n",
      "1203            11.0  \n",
      "2035            11.0  \n",
      "14              10.0  \n"
     ]
    }
   ],
   "source": [
    "# Group by 'author' and 'year' to count the number of submissions\n",
    "submission_counts = df_base_year_filtered_dataframes[f\"df_{base_year}_dog_submissions_filtered\"].groupby(\n",
    "    ['author', df_base_year_filtered_dataframes[f\"df_{base_year}_dog_submissions_filtered\"]['created_datetime'].dt.year]\n",
    ").size().reset_index(name='post_count')\n",
    "\n",
    "# Group by 'author' and 'year' to count the number of comments\n",
    "comment_counts = df_base_year_filtered_dataframes[f\"df_{base_year}_dog_comments_filtered\"].groupby(\n",
    "    ['author', df_base_year_filtered_dataframes[f\"df_{base_year}_dog_comments_filtered\"]['created_datetime'].dt.year]\n",
    ").size().reset_index(name='comment_count')\n",
    "\n",
    "# Merge the two datasets on 'author' and 'year' columns\n",
    "total_activity = pd.merge(\n",
    "    submission_counts,\n",
    "    comment_counts,\n",
    "    on=['author', 'created_datetime'],  # Adjust to merge on the correct column names if 'created_datetime' stores the year\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Replace NaN values with 0 in case there are authors who only posted or only commented\n",
    "total_activity.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the total activity for each user by summing posts and comments\n",
    "total_activity['total_activity'] = total_activity['post_count'] + total_activity['comment_count']\n",
    "\n",
    "# Sort the dataframe by 'total_activity' in descending order to get the most active users\n",
    "top_users = total_activity.sort_values(by='total_activity', ascending=False)\n",
    "\n",
    "# Print the top 20 most active users\n",
    "print(top_users.head(20))\n",
    "\n",
    "# Select all the top users\n",
    "top_20_users = top_users\n",
    "top_20_users_dog_2017 = top_20_users['author']\n",
    "top_20_combination_df = pd.DataFrame([(x, y) for x in top_20_users_dog_2017  for y in top_20_users_dog_2017  if x != y], columns=[\"Parent ID\", \"Child ID\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d996a",
   "metadata": {},
   "source": [
    "## 4. Reddit User Interaction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11722e9",
   "metadata": {},
   "source": [
    "### 4.1 Function: Process and Merge Comments and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a1b2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_merge_comments_submissions(comments_df, submissions_df):\n",
    "    # Extract year from 'created_datetime' column\n",
    "    comments_df['Year'] = pd.to_datetime(comments_df['created_datetime']).dt.year\n",
    "    submissions_df['Year'] = pd.to_datetime(submissions_df['created_datetime']).dt.year\n",
    "    \n",
    "    comments_df['noprefix_parent_id'] = comments_df['parent_id'].str[3:]\n",
    "    \n",
    "    mask_t3 = comments_df['parent_id'].str.startswith('t3')\n",
    "    df_t3 = comments_df[mask_t3].copy()\n",
    "    df_t3_merged = df_t3.merge(submissions_df[['id', 'author', 'Year']], left_on='noprefix_parent_id', right_on='id', suffixes=('_child', '_parent'))\n",
    "    df_t3_merged['Link Type'] = 'respond to a submission'\n",
    "\n",
    "    mask_t1 = comments_df['parent_id'].str.startswith('t1')\n",
    "    df_t1 = comments_df[mask_t1].copy()\n",
    "    df_t1_merged = df_t1.merge(comments_df[['id', 'author', 'Year']], left_on='noprefix_parent_id', right_on='id', suffixes=('_child', '_parent'))\n",
    "    df_t1_merged['Link Type'] = 'respond to a comment'\n",
    "\n",
    "    # Combine both sets of merged data\n",
    "    combined_df = pd.concat([df_t3_merged, df_t1_merged], ignore_index=True)\n",
    "    \n",
    "    # Choose the earliest year if there's a discrepancy between child and parent\n",
    "    combined_df['Interaction Year'] = combined_df[['Year_child', 'Year_parent']].min(axis=1)\n",
    "    \n",
    "    # Clean up the DataFrame\n",
    "    combined_df = combined_df.drop(columns=['Year_child', 'Year_parent'])\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95856040",
   "metadata": {},
   "source": [
    "### 4.2 Processing and Merging Comments and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de84bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframes = {}\n",
    "\n",
    "# List of subreddits\n",
    "subreddits = ['dog', 'pets', 'puppies', 'puppy_smiles']\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    comments_df = df_base_year_filtered_dataframes[f'df_2017_{subreddit}_comments_filtered']\n",
    "    submissions_df = df_base_year_filtered_dataframes[f'df_2017_{subreddit}_submissions_filtered']\n",
    "    final_dataframes[subreddit] = process_and_merge_comments_submissions(comments_df, submissions_df)\n",
    "\n",
    "# After processing, update the dataframes with the new structure including 'Interaction Year'\n",
    "for subreddit, df in final_dataframes.items():\n",
    "    df = df[['author_child', 'author_parent', 'Link Type', 'Interaction Year']]\n",
    "    df.columns = ['Child ID', 'Parent ID', 'Link Type', 'Year']\n",
    "    final_dataframes[subreddit] = df\n",
    "\n",
    "dog = final_dataframes['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8a593",
   "metadata": {},
   "source": [
    "### 4.3 Generating Tie Tables for 2017 and 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a0b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['Year'] = pd.to_datetime(comments_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submissions_df['Year'] = pd.to_datetime(submissions_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['noprefix_parent_id'] = comments_df['parent_id'].str[3:]\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['Year'] = pd.to_datetime(comments_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submissions_df['Year'] = pd.to_datetime(submissions_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['noprefix_parent_id'] = comments_df['parent_id'].str[3:]\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['Year'] = pd.to_datetime(comments_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submissions_df['Year'] = pd.to_datetime(submissions_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['noprefix_parent_id'] = comments_df['parent_id'].str[3:]\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['Year'] = pd.to_datetime(comments_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submissions_df['Year'] = pd.to_datetime(submissions_df['created_datetime']).dt.year\n",
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_16828\\2870910691.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_df['noprefix_parent_id'] = comments_df['parent_id'].str[3:]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent ID</th>\n",
       "      <th>Child ID</th>\n",
       "      <th>2017 Tie</th>\n",
       "      <th>2019 Tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--i_make_things--</td>\n",
       "      <td>-avi1998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--i_make_things--</td>\n",
       "      <td>-jinacio-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--i_make_things--</td>\n",
       "      <td>0010110012Amen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--i_make_things--</td>\n",
       "      <td>0010pond</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--i_make_things--</td>\n",
       "      <td>02tylerrobert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191366</th>\n",
       "      <td>zinzzzin</td>\n",
       "      <td>zoepeterson02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191367</th>\n",
       "      <td>zinzzzin</td>\n",
       "      <td>zww8169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191368</th>\n",
       "      <td>zlifea</td>\n",
       "      <td>zoepeterson02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191369</th>\n",
       "      <td>zlifea</td>\n",
       "      <td>zww8169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191370</th>\n",
       "      <td>zoepeterson02</td>\n",
       "      <td>zww8169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2191371 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Parent ID        Child ID  2017 Tie  2019 Tie\n",
       "0        --i_make_things--        -avi1998         0         0\n",
       "1        --i_make_things--       -jinacio-         0         0\n",
       "2        --i_make_things--  0010110012Amen         0         0\n",
       "3        --i_make_things--        0010pond         0         0\n",
       "4        --i_make_things--   02tylerrobert         0         0\n",
       "...                    ...             ...       ...       ...\n",
       "2191366           zinzzzin   zoepeterson02         0         0\n",
       "2191367           zinzzzin         zww8169         0         0\n",
       "2191368             zlifea   zoepeterson02         0         0\n",
       "2191369             zlifea         zww8169         0         0\n",
       "2191370      zoepeterson02         zww8169         0         0\n",
       "\n",
       "[2191371 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_2017 = top_20_combination_df.merge(dog, on=['Parent ID', 'Child ID'], how='left')\n",
    "merged_2017['binary indicator'] = merged_2017['Link Type'].notnull().astype(int)\n",
    "merged_2017 = merged_2017[['Parent ID', 'Child ID', 'binary indicator']]\n",
    "\n",
    "import numpy as np\n",
    "# Normalize the order of Parent ID and Child ID\n",
    "merged_2017[['Parent ID', 'Child ID']] = pd.DataFrame(np.sort(merged_2017[['Parent ID', 'Child ID']].values, axis=1), index=merged_2017.index)\n",
    "\n",
    "# Group by Parent ID and Child ID and aggregate the binary indicator\n",
    "merged_2017_group = merged_2017.groupby(['Parent ID', 'Child ID'], as_index=False)['binary indicator'].max()\n",
    "merged_2017_group.rename(columns={'binary indicator': '2017 Tie'}, inplace=True)\n",
    "merged_2017_group\n",
    "\n",
    "final_dataframes_2019 = {}\n",
    "\n",
    "# List of subreddits\n",
    "subreddits = ['dog', 'pets', 'puppies', 'puppy_smiles']\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    comments_df = df_comparison_year_filtered_dataframes[f'df_2019_{subreddit}_comments_filtered']\n",
    "    submissions_df = df_comparison_year_filtered_dataframes[f'df_2019_{subreddit}_submissions_filtered']\n",
    "    final_dataframes_2019[subreddit] = process_and_merge_comments_submissions(comments_df, submissions_df)\n",
    "\n",
    "# After processing, update the dataframes with the new structure including 'Interaction Year'\n",
    "for subreddit, df in final_dataframes_2019.items():\n",
    "    df = df[['author_child', 'author_parent', 'Link Type', 'Interaction Year']]\n",
    "    df.columns = ['Child ID', 'Parent ID', 'Link Type', 'Year']\n",
    "    final_dataframes_2019[subreddit] = df\n",
    "\n",
    "dog_2019 = final_dataframes_2019['dog']\n",
    "dog_2019\n",
    "\n",
    "merged_2019 = top_20_combination_df.merge(dog_2019, on=['Parent ID', 'Child ID'], how='left')\n",
    "merged_2019['binary indicator'] = merged_2019['Link Type'].notnull().astype(int)\n",
    "merged_2019 = merged_2019[['Parent ID', 'Child ID', 'binary indicator']]\n",
    "\n",
    "import numpy as np\n",
    "# Normalize the order of Parent ID and Child ID\n",
    "merged_2019[['Parent ID', 'Child ID']] = pd.DataFrame(np.sort(merged_2019[['Parent ID', 'Child ID']].values, axis=1), index=merged_2019.index)\n",
    "\n",
    "# Group by Parent ID and Child ID and aggregate the binary indicator\n",
    "merged_2019_group = merged_2019.groupby(['Parent ID', 'Child ID'], as_index=False)['binary indicator'].max()\n",
    "merged_2019_group.rename(columns={'binary indicator': '2019 Tie'}, inplace=True)\n",
    "merged_2019_group\n",
    "\n",
    "tie_table = merged_2017_group.merge(merged_2019_group, on=['Parent ID', 'Child ID'], how='left')\n",
    "tie_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fe2a5",
   "metadata": {},
   "source": [
    "### 4.4 Feature Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cda15e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>General Interest 2017</th>\n",
       "      <th>General Interest 2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tillmantino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010pond</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imguralbumbot</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crazydogsinfo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>codepro1989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>cocols</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>cnuss1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Ciliu</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>-avi1998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author  General Interest 2017  General Interest 2019\n",
       "0       Tillmantino                      0                      0\n",
       "1     AutoModerator                      0                      0\n",
       "2          0010pond                      0                      0\n",
       "3     imguralbumbot                      1                      1\n",
       "4     crazydogsinfo                      0                      0\n",
       "...             ...                    ...                    ...\n",
       "2089    codepro1989                      0                      0\n",
       "2090         cocols                      0                      0\n",
       "2091         cnuss1                      0                      0\n",
       "2092          Ciliu                      0                      0\n",
       "2093       -avi1998                      0                      0\n",
       "\n",
       "[2094 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Top_20_users_against_baseyear = top_20_users.copy()  # Create a copy of top_20_users\n",
    "\n",
    "# Initialize new columns with default value 0\n",
    "for df_name, df in df_base_year_filtered_dataframes.items():\n",
    "    if df_name not in [f\"df_{base_year}_dog_comments_filtered\", f\"df_{base_year}_dog_submissions_filtered\"]:\n",
    "         Top_20_users_against_baseyear[f'in_{df_name}'] = 0\n",
    "\n",
    "# Iterate through each user in top_20_users dataframe\n",
    "for index, row in top_20_users.iterrows():\n",
    "    author = row['author']\n",
    "    \n",
    "    # Check if the user is present in each dataframe\n",
    "    for df_name, df in df_base_year_filtered_dataframes.items():\n",
    "        if author in df['author'].values:\n",
    "            Top_20_users_against_baseyear.at[index, f'in_{df_name}'] = 1\n",
    "\n",
    "\n",
    "Top_20_users_against_baseyear['General Interest 2017'] = (Top_20_users_against_baseyear.iloc[:, 5:-2].any(axis=1)).astype(int)\n",
    "Top_20_users_against_comparisonyear = top_20_users.copy()  # Create a copy of top_20_users\n",
    "\n",
    "# Initialize new columns with default value 0\n",
    "for df_name, df in df_comparison_year_filtered_dataframes.items():\n",
    "         Top_20_users_against_comparisonyear[f'in_{df_name}'] = 0\n",
    "\n",
    "# Iterate through each user in top_20_users dataframe\n",
    "for index, row in top_20_users.iterrows():\n",
    "    author = row['author']\n",
    "    \n",
    "    # Check if the user is present in each dataframe\n",
    "    for df_name, df in df_comparison_year_filtered_dataframes.items():\n",
    "        if author in df['author'].values:\n",
    "            Top_20_users_against_comparisonyear.at[index, f'in_{df_name}'] = 1\n",
    "\n",
    "Top_20_users_against_comparisonyear['General Interest 2019'] = (Top_20_users_against_comparisonyear.iloc[:, 7:].any(axis=1)).astype(int)\n",
    "feature_table = Top_20_users_against_baseyear.merge(Top_20_users_against_comparisonyear, on='author', how='outer', suffixes=('_2017', '_2019'))\n",
    "feature_table = feature_table[['author', 'General Interest 2017', 'General Interest 2019']]\n",
    "\n",
    "feature_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46cface",
   "metadata": {},
   "source": [
    "## 5. User Interaction Analysis: Chi-Square Test for Association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f6e3f",
   "metadata": {},
   "source": [
    "This section of code conducts a chi-square test for association to analyze the relationship between the general interests of users in 2017 and 2019, considering the ties between users as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fedcf",
   "metadata": {},
   "source": [
    "### 5.1 Create combination dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d85804b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Create combinations DataFrame\n",
    "combinations_df = pd.DataFrame(list(combinations(feature_table['author'], 2)), columns=['Parent ID', 'Child ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73899826",
   "metadata": {},
   "source": [
    "### 5.2 Merge to Find **Shared** General Interests and Map Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c42bcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to find General Interests for both 'Parent ID' and 'Child ID'\n",
    "combi_table = combinations_df.merge(feature_table, left_on='Parent ID', right_on='author')\n",
    "combi_table = combi_table.merge(feature_table, left_on='Child ID', right_on='author', suffixes=('_Parent', '_Child'))\n",
    "\n",
    "# Calculate the sum of 'General Interest 2017' and 'General Interest 2019' for the new table\n",
    "combi_table['General Interest 2017'] = combi_table['General Interest 2017_Parent'] + combi_table['General Interest 2017_Child']\n",
    "combi_table['General Interest 2019'] = combi_table['General Interest 2019_Parent'] + combi_table['General Interest 2019_Child']\n",
    "\n",
    "\n",
    "# Function to map sums to 0 or 1 based on the described logic\n",
    "def map_interest(value):\n",
    "    return 1 if value in [0, 2] else 0\n",
    "\n",
    "# Apply the mapping function to the General Interest columns\n",
    "combi_table['General Interest 2017'] = combi_table['General Interest 2017'].apply(map_interest)\n",
    "combi_table['General Interest 2019'] = combi_table['General Interest 2019'].apply(map_interest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555937d5",
   "metadata": {},
   "source": [
    "### 5.3 Dataframe cleanup and final prep for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ffefe92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "combi_table = combi_table[['Parent ID', 'Child ID', 'General Interest 2017', 'General Interest 2019']]\n",
    "\n",
    "# Normalize 'Parent ID' and 'Child ID' order in both dataframes\n",
    "combi_table['sorted_id_1'] = combi_table.apply(lambda x: sorted([x['Parent ID'], x['Child ID']])[0], axis=1)\n",
    "combi_table['sorted_id_2'] = combi_table.apply(lambda x: sorted([x['Parent ID'], x['Child ID']])[1], axis=1)\n",
    "tie_table['sorted_id_1'] = tie_table.apply(lambda x: sorted([x['Parent ID'], x['Child ID']])[0], axis=1)\n",
    "tie_table['sorted_id_2'] = tie_table.apply(lambda x: sorted([x['Parent ID'], x['Child ID']])[1], axis=1)\n",
    "\n",
    "# Merge the tables using the sorted ID columns\n",
    "merged_table = combi_table.merge(tie_table, left_on=['sorted_id_1', 'sorted_id_2'], right_on=['sorted_id_1', 'sorted_id_2'], how='left')\n",
    "\n",
    "# Clean up: Drop the temporary sorted ID columns if no longer needed\n",
    "merged_table.drop(columns=['sorted_id_1', 'sorted_id_2'], inplace=True)\n",
    "\n",
    "# Handle missing values from the merge, if any (assuming missing ties are 0)\n",
    "merged_table.fillna({'2017 Tie': 0, '2019 Tie': 0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61b2c3",
   "metadata": {},
   "source": [
    "### 5.4 Chi-Square Tests and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20135551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of 'General Interest 2017' with tie in 2017:\n",
      " 1    489\n",
      "0    189\n",
      "Name: General Interest 2017, dtype: int64\n",
      "Value counts of 'General Interest 2017' without tie in 2017:\n",
      " 1    1725109\n",
      "0     465584\n",
      "Name: General Interest 2017, dtype: int64\n",
      "Value counts of 'General Interest 2017' with tie in 2019:\n",
      " 1    6\n",
      "0    1\n",
      "Name: General Interest 2017, dtype: int64\n",
      "Value counts of 'General Interest 2017' without tie in 2019:\n",
      " 1    1725592\n",
      "0     465772\n",
      "Name: General Interest 2017, dtype: int64\n",
      "Value counts of 'General Interest 2019' with tie in 2017:\n",
      " 1    583\n",
      "0     95\n",
      "Name: General Interest 2019, dtype: int64\n",
      "Value counts of 'General Interest 2019' without tie in 2017:\n",
      " 1    2114679\n",
      "0      76014\n",
      "Name: General Interest 2019, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze Results with Chi-Square Test\n",
    "# Value counts of 'General Interest 2017' when there is a tie in 2017\n",
    "print(\"Value counts of 'General Interest 2017' with tie in 2017:\\n\", merged_table[merged_table['2017 Tie'] == 1]['General Interest 2017'].value_counts())\n",
    "\n",
    "# Value counts of 'General Interest 2017' when there is no tie in 2017\n",
    "print(\"Value counts of 'General Interest 2017' without tie in 2017:\\n\", merged_table[merged_table['2017 Tie'] == 0]['General Interest 2017'].value_counts())\n",
    "\n",
    "# Value counts of 'General Interest 2017' when there is a tie in 2019\n",
    "print(\"Value counts of 'General Interest 2017' with tie in 2019:\\n\", merged_table[merged_table['2019 Tie'] == 1]['General Interest 2017'].value_counts())\n",
    "\n",
    "# Value counts of 'General Interest 2017' when there is no tie in 2019\n",
    "print(\"Value counts of 'General Interest 2017' without tie in 2019:\\n\", merged_table[merged_table['2019 Tie'] == 0]['General Interest 2017'].value_counts())\n",
    "\n",
    "# Value counts of 'General Interest 2019' when there is a tie in 2017\n",
    "print(\"Value counts of 'General Interest 2019' with tie in 2017:\\n\", merged_table[merged_table['2017 Tie'] == 1]['General Interest 2019'].value_counts())\n",
    "\n",
    "# Value counts of 'General Interest 2019' when there is no tie in 2017\n",
    "print(\"Value counts of 'General Interest 2019' without tie in 2017:\\n\", merged_table[merged_table['2017 Tie'] == 0]['General Interest 2019'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ed03bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic (Test 1): 17.371286521779087\n",
      "P-value (Test 1): 3.0743554227997474e-05\n",
      "Degrees of freedom (Test 1): 1\n",
      "Expected frequencies table (Test 1):\n",
      "[[5.33891999e+02 1.44108001e+02]\n",
      " [1.72506411e+06 4.65628892e+05]]\n",
      "\n",
      "\n",
      "Chi-square statistic (Test 2): 0.0\n",
      "P-value (Test 2): 1.0\n",
      "Degrees of freedom (Test 2): 1\n",
      "Expected frequencies table (Test 2):\n",
      "[[5.51215928e+00 1.48784072e+00]\n",
      " [1.72559249e+06 4.65771512e+05]]\n",
      "\n",
      "\n",
      "Chi-square statistic (Test 3): 221.5483001746039\n",
      "P-value (Test 3): 4.155900132350048e-50\n",
      "Degrees of freedom (Test 3): 1\n",
      "Expected frequencies table (Test 3):\n",
      "[[6.54452229e+02 2.35477708e+01]\n",
      " [2.11460755e+06 7.60854522e+04]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform chi-square tests\n",
    "from scipy.stats import chi2_contingency \n",
    "\n",
    "# First chi-square test\n",
    "observed_1 = [[489, 189], [1725109, 465584]] \n",
    "chi2_1, p_1, dof_1, expected_1 = chi2_contingency(observed_1)\n",
    "\n",
    "# Print results\n",
    "print(\"Chi-square statistic (Test 1):\", chi2_1)\n",
    "print(\"P-value (Test 1):\", p_1)\n",
    "print(\"Degrees of freedom (Test 1):\", dof_1)\n",
    "print(\"Expected frequencies table (Test 1):\")\n",
    "print(expected_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Second chi-square test\n",
    "observed_2 = [[6, 1], [1725592, 465772]] \n",
    "chi2_2, p_2, dof_2, expected_2 = chi2_contingency(observed_2)\n",
    "\n",
    "\n",
    "print(\"Chi-square statistic (Test 2):\", chi2_2)\n",
    "print(\"P-value (Test 2):\", p_2)\n",
    "print(\"Degrees of freedom (Test 2):\", dof_2)\n",
    "print(\"Expected frequencies table (Test 2):\")\n",
    "print(expected_2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Third chi-square test\n",
    "observed_3 = [[583, 95], [2114679, 76014]] \n",
    "chi2_3, p_3, dof_3, expected_3 = chi2_contingency(observed_3)\n",
    "\n",
    "print(\"Chi-square statistic (Test 3):\", chi2_3)\n",
    "print(\"P-value (Test 3):\", p_3)\n",
    "print(\"Degrees of freedom (Test 3):\", dof_3)\n",
    "print(\"Expected frequencies table (Test 3):\")\n",
    "print(expected_3)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
